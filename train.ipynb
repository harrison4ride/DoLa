{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/home/hxxzhang/DoLa/token_probabilities_dola.csv')\n",
    "# df = pd.concat([df, pd.read_csv('/home/hxxzhang/DoLa/token_probabilities_dola.csv')], axis=0)\n",
    "\n",
    "# Feature selection: Using 'top1_token_id' and 'top1_prob' from each premature stage\n",
    "features = [col for col in df.columns if 'token_id' in col or 'prob' in col]\n",
    "X = df[features]\n",
    "y = df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the probabilities\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, './model/scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9424223844824703\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.06      0.10      1087\n",
      "           1       0.94      1.00      0.97     16854\n",
      "\n",
      "    accuracy                           0.94     17941\n",
      "   macro avg       0.93      0.53      0.54     17941\n",
      "weighted avg       0.94      0.94      0.92     17941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/random_forest_model2.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, './model/random_forest_model2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9807956754706245\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      2835\n",
      "           1       0.98      1.00      0.99    144788\n",
      "\n",
      "    accuracy                           0.98    147623\n",
      "   macro avg       0.49      0.50      0.50    147623\n",
      "weighted avg       0.96      0.98      0.97    147623\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hxxzhang/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hxxzhang/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hxxzhang/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on recent\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/home/hxxzhang/DoLa/token_probabilities_recent.csv')\n",
    "features = [col for col in df.columns if 'token_id' in col or 'prob' in col]\n",
    "X = df[features]\n",
    "y = df['label']\n",
    "\n",
    "# Scaling the probabilities\n",
    "loaded_model = joblib.load('/home/hxxzhang/DoLa/model/random_forest_model2.pkl')\n",
    "# scaler = joblib.load('/home/hxxzhang/DoLa/model/scaler.pkl')\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "y_pred = loaded_model.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two csv files has overlap. Should remove the overlap.\n",
    "## No, it can not be removed. but there is data inbalance exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "class DoLaDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        if csv_path=='train':\n",
    "            df = pd.read_csv('/home/hxxzhang/DoLa/token_probabilities.csv')\n",
    "            df = pd.concat([df, pd.read_csv('/home/hxxzhang/DoLa/token_probabilities_dola.csv')], axis=0)\n",
    "        else:\n",
    "            df = pd.read_csv('/home/hxxzhang/DoLa/token_probabilities_recent.csv')\n",
    "        # Define layer names in order\n",
    "        premature_layers = [f'premature_{i}' for i in range(16, 32, 2)]\n",
    "        self.layer_names = premature_layers + ['mature']\n",
    "        self.num_layers = len(self.layer_names)\n",
    "        \n",
    "        layer_features = []\n",
    "        \n",
    "        # Process each layer\n",
    "        for layer_name in self.layer_names:\n",
    "            layer_data = []\n",
    "            # Get top-5 token IDs and probabilities for each layer\n",
    "            for k in range(1, 6):\n",
    "                token_id_col = f'{layer_name}_top{k}_token_id'\n",
    "                prob_col = f'{layer_name}_top{k}_prob'\n",
    "                \n",
    "                if token_id_col in df.columns and prob_col in df.columns:\n",
    "                    # Normalize token IDs by vocabulary size (assuming max token ID is 32000)\n",
    "                    token_ids = df[token_id_col].values / 32000.0\n",
    "                    probs = df[prob_col].values\n",
    "                    layer_data.extend([token_ids, probs])\n",
    "            \n",
    "            # Stack features for this layer\n",
    "            layer_features.append(np.stack(layer_data, axis=1))\n",
    "        \n",
    "        # Stack features for all layers\n",
    "        self.features = np.stack(layer_features, axis=1)  # Shape: [batch, num_layers, feature_dim]\n",
    "        self.labels = df['label'].values if 'label' in df.columns else None\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        if self.labels is not None:\n",
    "            self.labels = torch.LongTensor(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class DoLaTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, num_layers, nhead=4, num_encoder_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_projection = nn.Linear(feature_dim, dim_feedforward)\n",
    "        self.pos_encoder = PositionalEncoding(dim_feedforward)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_feedforward,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        \n",
    "        self.layer_attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim_feedforward,\n",
    "            num_heads=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Progressive depth weighting\n",
    "        self.depth_weights = nn.Parameter(torch.linspace(1.0, 2.0, num_layers))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim_feedforward, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_layers, feature_dim]\n",
    "        \n",
    "        # Project features\n",
    "        x = self.feature_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Apply depth-weighted attention\n",
    "        weights = self.depth_weights.softmax(dim=0)\n",
    "        x = x * weights.unsqueeze(0).unsqueeze(-1)\n",
    "        \n",
    "        # Self-attention across layers\n",
    "        x, _ = self.layer_attention(x, x, x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(x).squeeze(-1)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4, device='cuda'):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.detach().cpu().numpy())\n",
    "            train_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        train_auc = roc_auc_score(train_labels, train_preds)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_labels in val_loader:\n",
    "                batch_features = batch_features.to(device)\n",
    "                outputs = model(batch_features)\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(batch_labels.numpy())\n",
    "        \n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train AUC: {train_auc:.4f}')\n",
    "        print(f'Val AUC: {val_auc:.4f}, Best Val AUC: {best_val_auc:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if optimizer.param_groups[0]['lr'] < 1e-6:\n",
    "            print('Learning rate too small. Stopping training.')\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4\n",
    "    FEATURE_DIM = 10  # 5 token IDs + 5 probabilities per layer\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = DoLaDataset('train')\n",
    "    val_dataset = DoLaDataset('2')\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DoLaTransformer(\n",
    "        feature_dim=FEATURE_DIM,\n",
    "        num_layers=train_dataset.num_layers,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        dim_feedforward=256,\n",
    "        dropout=0.1\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in val_loader:\n",
    "            batch_features = batch_features.to(DEVICE)\n",
    "            outputs = model(batch_features)\n",
    "            val_preds.extend((outputs > 0.5).cpu().numpy())\n",
    "            val_labels.extend(batch_labels.numpy())\n",
    "    \n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolanew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
