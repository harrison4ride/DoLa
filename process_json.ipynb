{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./llama2-7b-recent-base.json','r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_package_data(package_name: str):\n",
    "    time.sleep(0.05)\n",
    "    url = f\"https://pypi.org/pypi/{package_name}/json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "import time\n",
    "saved = []\n",
    "for i in data['is_correct']:\n",
    "    cnt+=1\n",
    "    tmp=i['incorrect']\n",
    "    i['incorrect']=[]\n",
    "    for package in tmp:\n",
    "        if package.lower() in saved:\n",
    "            i['correct'].append(package)\n",
    "            continue\n",
    "        if get_package_data(package.lower()):\n",
    "            i['correct'].append(package)\n",
    "            saved.append(package.lower())\n",
    "        else:\n",
    "            i['incorrect'].append(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./llama2-7b-recent-base2.json', \"w\") as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Q:..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "with open('./code-llama-7b-all-base.json','r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.cookiejar\n",
    "\n",
    "# Initialize a session using urllib with cookie support\n",
    "cookie_jar = http.cookiejar.CookieJar()\n",
    "opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar))\n",
    "opener.addheaders = [\n",
    "    ('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36')\n",
    "]\n",
    "\n",
    "def get_package_data(package_name: str):\n",
    "    url = f\"https://pypi.org/pypi/{package_name}/json\"\n",
    "    try:\n",
    "        with opener.open(url) as response:\n",
    "            if response.getcode() == 200:\n",
    "                return True\n",
    "    except urllib.error.HTTPError as e:\n",
    "        if e.code == 404:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def is_correct(model_answer):\n",
    "    matches = re.findall(r'pip install ([a-zA-Z0-9_\\-]+)', model_answer)\n",
    "    result = {'correct':[],'incorrect':[]}\n",
    "    for package in matches:\n",
    "        if package.lower() in saved:\n",
    "            result['correct'].append(package)\n",
    "            continue\n",
    "        if get_package_data(package.lower()):\n",
    "            result['correct'].append(package)\n",
    "            saved.append(package.lower())\n",
    "        else:\n",
    "            result['incorrect'].append(package)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean']=[]\n",
    "saved = []\n",
    "for i in data['model_completion']:\n",
    "    i = i.split('Q: ')[0]\n",
    "    data['clean'].append(is_correct(i))\n",
    "with open('./code-llama-7b-all-base.json', \"w\") as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1959, 5035, 0.3890764647467726, 2449, 7769)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./llama2-7b-all.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2204, 5035, 0.4377358490566038, 2932, 8904)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./llama2-7b-all-base.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2043, 5064, 0.403436018957346, 2584, 8019)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./llama2-7b-recent.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 5064, 0.4443127962085308, 2947, 8100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./llama2-7b-recent-base.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1786, 5064, 0.35268562401263825, 2375, 10826)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# 176 case getting worse\n",
    "with open('./llama3-8b-recent.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1491, 5035, 0.2961271102284012, 1991, 8223)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# 176 case getting worse\n",
    "with open('./llama3-7b-ddola-all-test.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['is_correct']]), len(data['is_correct']), sum([i['incorrect']!=[] for i in data['is_correct']])/len(data['is_correct']), sum([len(i['incorrect']) for i in data['is_correct']]), sum([len(i['correct']) for i in data['is_correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17817913748617767, 0.17991061283236118, 0.1949285294693558)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2417/(2417+11148), 2375/(2375+10826), 1991/(1991+8223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 5064, 0.42022116903633494, 3350, 11524)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./llama3-8b-recent-base.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1345, 5035, 0.2671300893743793, 1737, 8585)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 410 case get incorrect after dola\n",
    "with open('./llama3-8b-all.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2105, 5035, 0.41807348560079444, 4157, 16091)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('./llama3-7b-pre.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['is_correct']]), len(data['is_correct']), sum([i['incorrect']!=[] for i in data['is_correct']])/len(data['is_correct']), sum([len(i['incorrect']) for i in data['is_correct']]), sum([len(i['correct']) for i in data['is_correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1360, 5035, 0.2701092353525323, 1771, 8571)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('./llama3-7b-test.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['is_correct']]), len(data['is_correct']), sum([i['incorrect']!=[] for i in data['is_correct']])/len(data['is_correct']), sum([len(i['incorrect']) for i in data['is_correct']]), sum([len(i['correct']) for i in data['is_correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1348, 5035, 0.26772591857000994, 1741, 8585)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('./llama3-7b-09.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['is_correct']]), len(data['is_correct']), sum([i['incorrect']!=[] for i in data['is_correct']])/len(data['is_correct']), sum([len(i['incorrect']) for i in data['is_correct']]), sum([len(i['correct']) for i in data['is_correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2148, 5035, 0.4266137040714995, 4331, 15452)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('./llama3-7b-base-test.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['is_correct']]), len(data['is_correct']), sum([i['incorrect']!=[] for i in data['is_correct']])/len(data['is_correct']), sum([len(i['incorrect']) for i in data['is_correct']]), sum([len(i['correct']) for i in data['is_correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1360, 5035, 0.2701092353525323, 1771, 8571)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# 176 case getting worse\n",
    "with open('./llama3-7b-test.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['is_correct']]), len(data['is_correct']), sum([i['incorrect']!=[] for i in data['is_correct']])/len(data['is_correct']), sum([len(i['incorrect']) for i in data['is_correct']]), sum([len(i['correct']) for i in data['is_correct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1935, 5035, 0.3843098311817279, 2969, 10007)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./llama3-8b-all-base.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 5035, 0.18430983118172792, 1109, 5859)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./code-llama-7b-all.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1363, 5035, 0.27070506454816284, 1871, 7646)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./code-llama-7b-all-base.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sum([i['incorrect']!=[] for i in data['clean']]), len(data['clean']), sum([i['incorrect']!=[] for i in data['clean']])/len(data['clean']), sum([len(i['incorrect']) for i in data['clean']]), sum([len(i['correct']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('./llama3-8b-all.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open('./llama3-8b-all-base.json','r') as file:\n",
    "    data2 = json.load(file)\n",
    "\n",
    "counter = 0\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\") # meta-llama/Llama-3.1-8B\n",
    "log_path = '/home/hxxzhang/DoLa/logs/llama3/python_all'\n",
    "\n",
    "res,cnt = [], -1\n",
    "for i in data['clean']:\n",
    "    # only check the case that did not passed in base model\n",
    "    cnt+=1\n",
    "    if len(data2['clean'][cnt]['incorrect'])==0:\n",
    "        packages = i['incorrect']\n",
    "        package_token_ids = {}\n",
    "        for package in packages:\n",
    "            tokens = tokenizer.tokenize(package)\n",
    "            token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            package_token_ids[package] = token_ids\n",
    "        with open(os.path.join(log_path,'log_'+str(cnt))) as f:\n",
    "            log_data = f.read()\n",
    "        layers = parse_log(log_data, package_token_ids)\n",
    "        res.append(analyze2(layers, package_token_ids))\n",
    "\n",
    "sum([len(i) for i in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def extract_pip_packages(model_output):\n",
    "    \"\"\"\n",
    "    Extract pip package names from model output.\n",
    "    \"\"\"\n",
    "    packages = re.findall(r'pip install ([a-zA-Z0-9_\\-\\.]+)', model_output)\n",
    "    return packages\n",
    "\n",
    "def process_layer_data(tokens_info, prefix, max_tokens=5):\n",
    "    \"\"\"\n",
    "    Process layer data and create a dictionary with top token information.\n",
    "    \"\"\"\n",
    "    sorted_tokens = sorted(tokens_info, key=lambda x: x['probability'], reverse=True)\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(min(max_tokens, len(sorted_tokens))):\n",
    "        token = sorted_tokens[i]\n",
    "        results[f'{prefix}_top{i+1}_token_id'] = token['id']\n",
    "        results[f'{prefix}_top{i+1}_prob'] = token['probability']\n",
    "        results[f'{prefix}_top{i+1}_token'] = token['token']  # Also store token text\n",
    "    \n",
    "    for i in range(len(sorted_tokens), max_tokens):\n",
    "        results[f'{prefix}_top{i+1}_token_id'] = None\n",
    "        results[f'{prefix}_top{i+1}_prob'] = None\n",
    "        results[f'{prefix}_top{i+1}_token'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_token_info(lines, start_idx):\n",
    "    \"\"\"\n",
    "    Extract token information from multiple lines, handling various newline patterns.\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "    i = start_idx\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        if line.startswith('Token:') and len(buffer) > 0:\n",
    "            break\n",
    "        if (line.startswith('Premature Layer') or \n",
    "            line.startswith('Mature Layer') or \n",
    "            'next_tokens' in line or\n",
    "            line.startswith('MODEL OUTPUT:')):\n",
    "            break\n",
    "            \n",
    "        if line:\n",
    "            buffer.append(line)\n",
    "        elif buffer and buffer[-1].strip():\n",
    "            buffer.append(line)\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "        full_text = ' '.join(buffer)\n",
    "        if '(ID:' in full_text and 'Probability:' in full_text:\n",
    "            match = re.search(r'Token:(.*?)\\(ID: (\\d+)\\), Probability: ([\\d\\.]+)', full_text)\n",
    "            if match:\n",
    "                token_text = match.group(1).strip()\n",
    "                token_id = int(match.group(2))\n",
    "                probability = float(match.group(3))\n",
    "                return {\n",
    "                    'token': token_text,\n",
    "                    'id': token_id,\n",
    "                    'probability': probability\n",
    "                }, i\n",
    "    \n",
    "    return None, i\n",
    "\n",
    "def extract_model_output(log_data):\n",
    "    \"\"\"\n",
    "    Extract the MODEL OUTPUT section from the log.\n",
    "    \"\"\"\n",
    "    start_match = re.search(r'MODEL OUTPUT:\\s*\\n', log_data)\n",
    "    end_match = re.search(r'MODEL OUTPUT END', log_data)\n",
    "    \n",
    "    if start_match and end_match:\n",
    "        return log_data[start_match.end():end_match.start()].strip()\n",
    "    return \"\"\n",
    "\n",
    "def parse_and_save_logs(log_data, target_strings, rows):\n",
    "    \"\"\"\n",
    "    Parse log data and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        log_data: String containing the log data\n",
    "        output_file: Path to the output CSV file\n",
    "        target_strings: List of target strings to match against token text\n",
    "    \"\"\"\n",
    "    current_row = {}\n",
    "    current_layer = None\n",
    "    \n",
    "    # Extract MODEL OUTPUT and pip packages\n",
    "    model_output = extract_model_output(log_data)\n",
    "    pip_packages = extract_pip_packages(model_output)\n",
    "    \n",
    "    lines = log_data.split('\\n')\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Check for Premature Layer\n",
    "        premature_match = re.match(r'Premature Layer (\\d+),(\\d+)', line)\n",
    "        if premature_match:\n",
    "            layer_num = premature_match.group(1)\n",
    "            sublayer_num = premature_match.group(2)\n",
    "            current_layer = f'premature_{layer_num}'\n",
    "            current_row[current_layer] = []\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        # Check for Mature Layer\n",
    "        if line.startswith('Mature Layer:'):\n",
    "            current_layer = 'mature'\n",
    "            current_row[current_layer] = []\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        # Parse token information\n",
    "        if line.startswith('Token:') and current_layer is not None:\n",
    "            token_info, next_idx = extract_token_info(lines, i)\n",
    "            if token_info:\n",
    "                current_row[current_layer].append(token_info)\n",
    "            i = next_idx\n",
    "            continue\n",
    "            \n",
    "        # Check for sequence end\n",
    "        if 'next_tokens' in line:\n",
    "            row_data = {}\n",
    "            for layer_name, tokens in current_row.items():\n",
    "                if tokens:\n",
    "                    row_data.update(process_layer_data(tokens, layer_name))\n",
    "            \n",
    "            if row_data and 'mature_top1_token' in row_data:\n",
    "                # Set default label to 1\n",
    "                row_data['label'] = 1\n",
    "                \n",
    "                # Check if mature top1 token matches any target string\n",
    "                mature_top1_token = row_data['mature_top1_token']\n",
    "                if mature_top1_token:\n",
    "                    # Clean and normalize token text for comparison\n",
    "                    clean_token = mature_top1_token.strip().lower()\n",
    "                    \n",
    "                    # Check if the token matches any target string\n",
    "                    for target in target_strings:\n",
    "                        if target.lower() in clean_token:\n",
    "                            # Check if this token appears in a pip install command\n",
    "                            for package in pip_packages:\n",
    "                                if target.lower() in package.lower():\n",
    "                                    row_data['label'] = 0\n",
    "                                    break\n",
    "                \n",
    "                rows.append(row_data)\n",
    "            \n",
    "            current_row = {}\n",
    "            current_layer = None\n",
    "        \n",
    "        i += 1\n",
    "    return rows\n",
    "import os\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    with open('./llama3-8b-all.json','r') as file:\n",
    "        data = json.load(file)\n",
    "    log_path = '/home/hxxzhang/DoLa/logs/llama3/python_all'\n",
    "    cnt = -1\n",
    "    rows = []\n",
    "    for i in data['clean']:\n",
    "        cnt+=1\n",
    "        # Tokens that do not do DoLa/Originally correct \n",
    "        target_strings = data['clean'][cnt]['incorrect']\n",
    "        with open(os.path.join(log_path,'log_'+str(cnt))) as f:\n",
    "            log_data = f.read()\n",
    "        rows = parse_and_save_logs(log_data, target_strings, rows)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv('token_probabilities_dola.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of processed data:\n",
      "   premature_16_top1_token_id  premature_16_top1_prob premature_16_top1_token  \\\n",
      "0                         220                0.000017                           \n",
      "1                         231                0.000017                       �   \n",
      "2                       78685                0.000016                     #ab   \n",
      "3                       43556                0.000018                UpInside   \n",
      "4                       43556                0.000023                UpInside   \n",
      "\n",
      "   premature_16_top2_token_id  premature_16_top2_prob premature_16_top2_token  \\\n",
      "0                       26601                0.000015            specialchars   \n",
      "1                       97405                0.000017                 ISMATCH   \n",
      "2                       85521                0.000015                     #aa   \n",
      "3                       59700                0.000018                  ILLISE   \n",
      "4                       59700                0.000018                  ILLISE   \n",
      "\n",
      "   premature_16_top3_token_id  premature_16_top3_prob premature_16_top3_token  \\\n",
      "0                       62102                0.000014            )application   \n",
      "1                       69688                0.000016               |required   \n",
      "2                      112996                0.000015                   andaş   \n",
      "3                       77781                0.000018                     ząd   \n",
      "4                       29233                0.000018                    -wsj   \n",
      "\n",
      "   premature_16_top4_token_id  ...  mature_top3_token_id mature_top3_prob  \\\n",
      "0                       17110  ...                  1179         0.016859   \n",
      "1                       16438  ...                 58999         0.000163   \n",
      "2                       14166  ...                 10344         0.005737   \n",
      "3                       28345  ...                  4120         0.002560   \n",
      "4                       98809  ...                   198         0.136697   \n",
      "\n",
      "   mature_top3_token  mature_top4_token_id mature_top4_prob  \\\n",
      "0             import                  1795         0.016293   \n",
      "1           -install                 71420         0.000149   \n",
      "2             python                 20104         0.004528   \n",
      "3             server                    11         0.001652   \n",
      "4                                   128001         0.087497   \n",
      "\n",
      "   mature_top4_token  mature_top5_token_id mature_top5_prob  \\\n",
      "0               http                   364         0.014131   \n",
      "1           installs                  1160         0.000091   \n",
      "2              flask                 25057         0.003072   \n",
      "3                  ,                  3622         0.001299   \n",
      "4    <|end_of_text|>                  1432         0.033967   \n",
      "\n",
      "   mature_top5_token  label  \n",
      "0                  '      1  \n",
      "1               list      1  \n",
      "2             urllib      0  \n",
      "3             server      1  \n",
      "4                         1  \n",
      "\n",
      "[5 rows x 136 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def extract_pip_packages(model_output):\n",
    "    \"\"\"\n",
    "    Extract pip package names from model output.\n",
    "    \"\"\"\n",
    "    packages = re.findall(r'pip install ([a-zA-Z0-9_\\-\\.]+)', model_output)\n",
    "    return packages\n",
    "\n",
    "def process_layer_data(tokens_info, prefix, max_tokens=5):\n",
    "    \"\"\"\n",
    "    Process layer data and create a dictionary with top token information.\n",
    "    \"\"\"\n",
    "    sorted_tokens = sorted(tokens_info, key=lambda x: x['probability'], reverse=True)\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(min(max_tokens, len(sorted_tokens))):\n",
    "        token = sorted_tokens[i]\n",
    "        results[f'{prefix}_top{i+1}_token_id'] = token['id']\n",
    "        results[f'{prefix}_top{i+1}_prob'] = token['probability']\n",
    "        results[f'{prefix}_top{i+1}_token'] = token['token']  # Also store token text\n",
    "    \n",
    "    for i in range(len(sorted_tokens), max_tokens):\n",
    "        results[f'{prefix}_top{i+1}_token_id'] = None\n",
    "        results[f'{prefix}_top{i+1}_prob'] = None\n",
    "        results[f'{prefix}_top{i+1}_token'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_token_info(lines, start_idx):\n",
    "    \"\"\"\n",
    "    Extract token information from multiple lines, handling various newline patterns.\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "    i = start_idx\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        if line.startswith('Token:') and len(buffer) > 0:\n",
    "            break\n",
    "        if (line.startswith('Premature Layer') or \n",
    "            line.startswith('Mature Layer') or \n",
    "            'next_tokens' in line or\n",
    "            line.startswith('MODEL OUTPUT:')):\n",
    "            break\n",
    "            \n",
    "        if line:\n",
    "            buffer.append(line)\n",
    "        elif buffer and buffer[-1].strip():\n",
    "            buffer.append(line)\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "        full_text = ' '.join(buffer)\n",
    "        if '(ID:' in full_text and 'Probability:' in full_text:\n",
    "            match = re.search(r'Token:(.*?)\\(ID: (\\d+)\\), Probability: ([\\d\\.]+)', full_text)\n",
    "            if match:\n",
    "                token_text = match.group(1).strip()\n",
    "                token_id = int(match.group(2))\n",
    "                probability = float(match.group(3))\n",
    "                return {\n",
    "                    'token': token_text,\n",
    "                    'id': token_id,\n",
    "                    'probability': probability\n",
    "                }, i\n",
    "    \n",
    "    return None, i\n",
    "\n",
    "def extract_model_output(log_data):\n",
    "    \"\"\"\n",
    "    Extract the MODEL OUTPUT section from the log.\n",
    "    \"\"\"\n",
    "    start_match = re.search(r'MODEL OUTPUT:\\s*\\n', log_data)\n",
    "    end_match = re.search(r'MODEL OUTPUT END', log_data)\n",
    "    \n",
    "    if start_match and end_match:\n",
    "        return log_data[start_match.end():end_match.start()].strip()\n",
    "    return \"\"\n",
    "\n",
    "def parse_and_save_logs(log_data, output_file, target_strings, rows):\n",
    "    \"\"\"\n",
    "    Parse log data and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        log_data: String containing the log data\n",
    "        output_file: Path to the output CSV file\n",
    "        target_strings: List of target strings to match against token text\n",
    "    \"\"\"\n",
    "    current_row = {}\n",
    "    current_layer = None\n",
    "    \n",
    "    # Extract MODEL OUTPUT and pip packages\n",
    "    model_output = extract_model_output(log_data)\n",
    "    pip_packages = extract_pip_packages(model_output)\n",
    "    \n",
    "    lines = log_data.split('\\n')\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Check for Premature Layer\n",
    "        premature_match = re.match(r'Premature Layer (\\d+),(\\d+)', line)\n",
    "        if premature_match:\n",
    "            layer_num = premature_match.group(1)\n",
    "            sublayer_num = premature_match.group(2)\n",
    "            current_layer = f'premature_{layer_num}'\n",
    "            current_row[current_layer] = []\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        # Check for Mature Layer\n",
    "        if line.startswith('Mature Layer:'):\n",
    "            current_layer = 'mature'\n",
    "            current_row[current_layer] = []\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        # Parse token information\n",
    "        if line.startswith('Token:') and current_layer is not None:\n",
    "            token_info, next_idx = extract_token_info(lines, i)\n",
    "            if token_info:\n",
    "                current_row[current_layer].append(token_info)\n",
    "            i = next_idx\n",
    "            continue\n",
    "            \n",
    "        # Check for sequence end\n",
    "        if 'next_tokens' in line:\n",
    "            row_data = {}\n",
    "            for layer_name, tokens in current_row.items():\n",
    "                if tokens:\n",
    "                    row_data.update(process_layer_data(tokens, layer_name))\n",
    "            \n",
    "            if row_data and 'mature_top1_token' in row_data:\n",
    "                # Set default label to 1\n",
    "                row_data['label'] = 1\n",
    "                \n",
    "                # Check if mature top1 token matches any target string\n",
    "                mature_top1_token = row_data['mature_top1_token']\n",
    "                if mature_top1_token:\n",
    "                    # Clean and normalize token text for comparison\n",
    "                    clean_token = mature_top1_token.strip().lower()\n",
    "                    \n",
    "                    # Check if the token matches any target string\n",
    "                    for target in target_strings:\n",
    "                        if target.lower() in clean_token:\n",
    "                            # Check if this token appears in a pip install command\n",
    "                            for package in pip_packages:\n",
    "                                if target.lower() in package.lower():\n",
    "                                    row_data['label'] = 0\n",
    "                                    break\n",
    "                \n",
    "                rows.append(row_data)\n",
    "            \n",
    "            current_row = {}\n",
    "            current_layer = None\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Convert to DataFrame and save to CSV\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    target_strings = ['http']\n",
    "    with open('/home/hxxzhang/DoLa/logs/llama3/all-base/log_0') as f:\n",
    "        log_data = f.read()\n",
    "    rows=[]\n",
    "    df = parse_and_save_logs(log_data, 'token_probabilities_dola.csv', target_strings,rows)\n",
    "    print(\"Sample of processed data:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res,cnt = [], -1\n",
    "for i in data['clean']:\n",
    "    # only check the case that did not passed in base model\n",
    "    cnt+=1\n",
    "    packages = i['incorrect']\n",
    "    package_token_ids = {}\n",
    "    for package in packages:\n",
    "        tokens = tokenizer.tokenize(package)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        package_token_ids[package] = token_ids\n",
    "    with open(os.path.join(log_path,'log_'+str(cnt))) as f:\n",
    "        log_data = f.read()\n",
    "    layers = parse_log(log_data, package_token_ids)\n",
    "    res.append(analyze2(layers, package_token_ids))\n",
    "sum([len(i) for i in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(367, 1345, 1737)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "cnt=0\n",
    "\n",
    "with open('./llama3-8b-all.json','r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "res = []\n",
    "for i in data['clean']:\n",
    "    if i['correct']!=[] and i['incorrect']!=[]:\n",
    "        res.append([data['model_completion'][cnt],i['correct'],i['incorrect']])\n",
    "    cnt+=1\n",
    "res\n",
    "\n",
    "def count_incorrect_after_correct(data_list):\n",
    "    count = 0\n",
    "\n",
    "    for data in data_list:\n",
    "        model_completion, correct, incorrect = data\n",
    "        correct_positions = []\n",
    "        incorrect_positions = []\n",
    "\n",
    "        # Find positions of correct packages in the model_completion string\n",
    "        for package in correct:\n",
    "            pos = model_completion.find(package)\n",
    "            if pos != -1:\n",
    "                correct_positions.append(pos)\n",
    "\n",
    "        # Find positions of incorrect packages in the model_completion string\n",
    "        for package in incorrect:\n",
    "            pos = model_completion.find(package)\n",
    "            if pos != -1:\n",
    "                incorrect_positions.append(pos)\n",
    "\n",
    "        # If any incorrect package appears after all correct packages\n",
    "        if incorrect_positions and correct_positions:\n",
    "            if min(incorrect_positions) > max(correct_positions):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "count_incorrect_after_correct(res),sum([1 for i in data['clean'] if i['incorrect']!=[]]),sum([len(i['incorrect']) for i in data['clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log(log_data, target_token_ids):\n",
    "    occurrences = defaultdict(list)  # key: token_id (as string), value: list of occurrences\n",
    "    current_sequence = {'layers': {}}\n",
    "    current_layer = None\n",
    "    for line in log_data.strip().split('\\n'):\n",
    "        line = line.strip()\n",
    "        # Check for Premature Layer\n",
    "        match = re.match(r'Premature Layer (\\d+), Sequence (\\d+):', line)\n",
    "        if match:\n",
    "            layer_num = int(match.group(1))\n",
    "            current_layer = layer_num\n",
    "            current_sequence['layers'][current_layer] = []\n",
    "            continue\n",
    "        # Check for Mature Layer\n",
    "        if line.startswith('Mature Layer:'):\n",
    "            current_layer = 'mature'\n",
    "            current_sequence['layers'][current_layer] = []\n",
    "            continue\n",
    "        # Check for tokens\n",
    "        match = re.match(r'Token: (.+) \\(ID: (\\d+)\\), Probability: ([\\d\\.]+)', line)\n",
    "        if match and current_layer is not None:\n",
    "            token = match.group(1)\n",
    "            token_id = int(match.group(2))\n",
    "            probability = float(match.group(3))\n",
    "            current_sequence['layers'][current_layer].append({\n",
    "                'token': token,\n",
    "                'id': token_id,\n",
    "                'probability': probability\n",
    "            })\n",
    "            continue\n",
    "        # Check for next_tokens\n",
    "        match = re.match(r'next_tokens, tensor\\(\\[(\\d+)\\], device', line)\n",
    "        if match:\n",
    "            next_token_id = int(match.group(1))\n",
    "            current_sequence['next_token_id'] = next_token_id\n",
    "            # If the next_token_id is in target_token_ids, we store the data\n",
    "            if next_token_id in sum([i for i in target_token_ids.values()],[]):\n",
    "                # For this occurrence, collect data per layer\n",
    "                occurrence_data = {}\n",
    "                layers = current_sequence['layers']\n",
    "                for layer_num, tokens in layers.items():\n",
    "                    # Record all tokens in this layer\n",
    "                    token_list = []\n",
    "                    for token_info in tokens:\n",
    "                        token_list.append({\n",
    "                            'token': token_info['token'],\n",
    "                            'id': token_info['id'],\n",
    "                            'probability': token_info['probability']\n",
    "                        })\n",
    "                    occurrence_data[layer_num] = token_list\n",
    "                occurrences[str(next_token_id)].append(occurrence_data)\n",
    "            # Reset current_sequence for the next sequence\n",
    "            current_sequence = {'layers': {}}\n",
    "            current_layer = None\n",
    "            continue\n",
    "    return occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(occurrences, target_tokens):\n",
    "    results = []\n",
    "    # Mapping from token_id to token name for easy reference\n",
    "    token_id_to_name = {}\n",
    "    for name, ids in target_tokens.items():\n",
    "        for id_ in ids:\n",
    "            token_id_to_name[str(id_)] = name\n",
    "    for token_id_str, occurrence_list in occurrences.items():\n",
    "        token_id = int(token_id_str)\n",
    "        token_name = token_id_to_name.get(token_id_str, f'ID {token_id_str}')\n",
    "        for idx, occurrence in enumerate(occurrence_list, 1):\n",
    "            layers = occurrence.keys()\n",
    "            # Get the mature layer tokens\n",
    "            mature_tokens = occurrence.get('mature', [])\n",
    "            if not mature_tokens:\n",
    "                continue\n",
    "            highest_mature_token = mature_tokens[0]\n",
    "            # Condition 1: The highest probability token in mature layer is not the token\n",
    "            if highest_mature_token['id'] == token_id:\n",
    "                continue  # Skip if the highest probability token is the token itself\n",
    "            # Collect probabilities of the token across premature layers\n",
    "            premature_layers = [layer for layer in occurrence if layer != 'mature']\n",
    "            premature_layers.sort()\n",
    "            probabilities = []\n",
    "            for layer_num in premature_layers:\n",
    "                tokens = occurrence[layer_num]\n",
    "                # Find the token in this layer\n",
    "                token_info = next((t for t in tokens if t['id'] == token_id), None)\n",
    "                if token_info:\n",
    "                    probabilities.append(token_info['probability'])\n",
    "                else:\n",
    "                    probabilities.append(0.0)\n",
    "            # Condition 2: Probability is rising in premature layers\n",
    "            if all(x <= y for x, y in zip(probabilities, probabilities[1:])):\n",
    "                # Both conditions met, add to results\n",
    "                if token_id_str not in results:\n",
    "                    results.append(token_id_str)\n",
    "    return results\n",
    "\n",
    "# If the next_token is different from the dola\n",
    "def analyze2(occurrences, target_tokens):\n",
    "    results = []\n",
    "    # Mapping from token_id to token name for easy reference\n",
    "    token_id_to_name = {}\n",
    "    for name, ids in target_tokens.items():\n",
    "        for id_ in ids:\n",
    "            token_id_to_name[str(id_)] = name\n",
    "    for token_id_str, occurrence_list in occurrences.items():\n",
    "        token_id = int(token_id_str)\n",
    "        token_name = token_id_to_name.get(token_id_str, f'ID {token_id_str}')\n",
    "        for idx, occurrence in enumerate(occurrence_list, 1):\n",
    "            layers = occurrence.keys()\n",
    "            # Get the mature layer tokens\n",
    "            mature_tokens = occurrence.get('mature', [])\n",
    "            if not mature_tokens:\n",
    "                continue\n",
    "            highest_mature_token = mature_tokens[0]\n",
    "            # Condition 1: The highest probability token in mature layer is not the token\n",
    "            if highest_mature_token['id'] == token_id:\n",
    "                continue  # Skip if the highest probability token is the token itself\n",
    "            if token_id_str not in results:\n",
    "                results.append(token_id_str)\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolanew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
